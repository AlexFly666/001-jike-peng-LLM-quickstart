{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a1d913f3-3c42-428e-9cc0-8d679d51897a",
      "metadata": {
        "id": "a1d913f3-3c42-428e-9cc0-8d679d51897a"
      },
      "source": [
        "# PEFT 库 QLoRA 实战 - ChatGLM3-6B\n",
        "\n",
        "通常，模型被量化后不会进一步训练用于下游任务，因为由于权重和激活的较低精度，训练可能不稳定。\n",
        "\n",
        "但是由于PEFT方法只添加额外的可训练参数，这使得我们可以使用PEFT适配器（Adapter）来训练一个量化模型！将量化与PEFT结合起来可以成为在单个GPU上训练大模型的微调策略。\n",
        "\n",
        "例如，`QLoRA` 是一种将模型量化为4位然后使用LoRA进行训练的方法，使得在单个16GB GPU（本教程以 NVIDIA T4为例）上微调一个具有65B参数的大模型成为可能。\n",
        "\n",
        "THUDM Hugging Face 主页：https://huggingface.co/THUDM\n",
        "\n",
        "## 教程说明\n",
        "\n",
        "本教程使用 QLoRA 论文中介绍的量化技术：`NF4 数据类型`、`双量化` 和 `混合精度计算`，在 `ChatGLM3-6b` 模型上实现了 QLoRA 微调。并展示了完整的 QLoRA 微调流程，具体如下：\n",
        "\n",
        "- 数据准备\n",
        "    - 下载数据集\n",
        "    - 设计 Tokenizer 函数处理样本（map、shuffle、flatten）\n",
        "    - 自定义批量数据处理类 DataCollatorForChatGLM\n",
        "- 训练模型\n",
        "    - 加载 ChatGLM3-6B 量化模型\n",
        "    - PEFT 量化模型预处理（prepare_model_for_kbit_training）\n",
        "    - QLoRA 适配器配置（TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING）\n",
        "    - 微调训练超参数配置（TrainingArguments）\n",
        "    - 开启训练（trainer.train)\n",
        "    - 保存QLoRA模型（trainer.model.save_pretrained)\n",
        "- [模型推理](peft_chatglm_inference.ipynb)\n",
        "    - 加载 ChatGLM3-6B 基础模型\n",
        "    - 加载 ChatGLM3-6B QLoRA 模型（PEFT Adapter）\n",
        "    - 微调前后对比"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.3.1\\\n",
        "transformers==4.37.2\\\n",
        "datasets==2.16.1\\\n",
        "scikit-learn==1.3.2\\\n",
        "peft==0.7.1"
      ],
      "metadata": {
        "id": "5f4TQXKSxlma",
        "outputId": "d85dbef8-3a85-4f09-ae35-bf46dc6460ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "5f4TQXKSxlma",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==2.3.1 in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: transformers==4.37.2 in /usr/local/lib/python3.10/dist-packages (4.37.2)\n",
            "Collecting datasets==2.16.1\n",
            "  Using cached datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: scikit-learn==1.3.2 in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: peft==0.7.1 in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (2023.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (2.3.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1) (3.11.10)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.2) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.2) (3.5.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.7.1) (5.9.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.1) (1.2.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1) (12.6.85)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.1) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.16.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.16.1) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.16.1) (2024.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.1) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.1) (1.17.0)\n",
            "Using cached datasets-2.16.1-py3-none-any.whl (507 kB)\n",
            "Installing collected packages: datasets\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 3.2.0\n",
            "    Uninstalling datasets-3.2.0:\n",
            "      Successfully uninstalled datasets-3.2.0\n",
            "Successfully installed datasets-2.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7fa8105c-6dda-426b-9180-ab9abbc9ce9e",
      "metadata": {
        "id": "7fa8105c-6dda-426b-9180-ab9abbc9ce9e"
      },
      "outputs": [],
      "source": [
        "# 定义全局变量和参数\n",
        "model_name_or_path = 'THUDM/chatglm3-6b'  # 模型ID或本地路径\n",
        "train_data_path = 'HasturOfficial/adgen'    # 训练数据路径\n",
        "eval_data_path = None                     # 验证数据路径，如果没有则设置为None\n",
        "seed = 8                                 # 随机种子\n",
        "max_input_length = 512                    # 输入的最大长度\n",
        "max_output_length = 1536                  # 输出的最大长度\n",
        "lora_rank = 4                             # LoRA秩\n",
        "lora_alpha = 32                           # LoRA alpha值\n",
        "lora_dropout = 0.05                       # LoRA Dropout率\n",
        "resume_from_checkpoint = None             # 如果从checkpoint恢复训练，指定路径\n",
        "prompt_text = ''                          # 所有数据前的指令文本\n",
        "compute_dtype = 'fp32'                    # 计算数据类型（fp32, fp16, bf16）"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93eab798-4ec0-45f0-af87-a3aa880f0888",
      "metadata": {
        "id": "93eab798-4ec0-45f0-af87-a3aa880f0888"
      },
      "source": [
        "## 数据准备\n",
        "\n",
        "### 下载数据集\n",
        "\n",
        "从 Hugging Face 加载 adgen 数据集，并tokenize，shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f7517070-eae3-45e1-b6a2-f49f332650ac",
      "metadata": {
        "id": "f7517070-eae3-45e1-b6a2-f49f332650ac",
        "outputId": "ff1292ca-e800-48d2-d11f-3224d95e9625",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(train_data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cbd03e18-05b7-47c3-bb32-520fb6f0bef3",
      "metadata": {
        "id": "cbd03e18-05b7-47c3-bb32-520fb6f0bef3",
        "outputId": "429f0565-a308-4b30-bec6-80be76d1f0cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['content', 'summary'],\n",
              "        num_rows: 114599\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['content', 'summary'],\n",
              "        num_rows: 1070\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ea17c452-be25-4424-884d-14ba92c17b79",
      "metadata": {
        "id": "ea17c452-be25-4424-884d-14ba92c17b79"
      },
      "outputs": [],
      "source": [
        "from datasets import ClassLabel, Sequence\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "# show_random_elements 函数的功能是从给定的数据集中随机抽取一些样本，并将这些样本以表格的形式展示出来。\n",
        "# 它可以帮助用户快速了解数据集的内容和结构。\n",
        "def show_random_elements(dataset, num_examples=10):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "\n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
        "    display(HTML(df.to_html()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7b4ee2cc-e964-4a47-9b18-67f99d65ff73",
      "metadata": {
        "id": "7b4ee2cc-e964-4a47-9b18-67f99d65ff73",
        "outputId": "5bb498f8-1d53-4dd0-cf92-9b5d5c0995be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>类型#裤*版型#宽松*版型#显瘦*风格#街头*风格#休闲*裤型#直筒裤*裤腰型#松紧腰</td>\n",
              "      <td>帆布鞋很受年轻人的喜爱，那么这款以帆布为面料制成的帆布裤呢，不知是否&lt;UNK&gt;成为街头的新宠。其大胆的宽松直筒版型，设计较为个性，直筒的裤子最大的特点就是显瘦。而松紧腰的添加可以说是让整款裤子的休闲感提升不少，穿脱也变得容易方便起来。</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>类型#上衣*图案#印花*图案#撞色*衣样式#衬衫*衣门襟#系带*衣款式#拼接*衣款式#绑带</td>\n",
              "      <td>&lt;UNK&gt;图案撞色衬衫系带裙，结合了个性图案设计，将衬衫和印花相拼接，拼接抹胸的设计更显露一丝女人味。前后绑带的设计，个性玩味，可以打结，可爱俏皮，可以随意垂落，率性十足，并且使整个服装更加立体，层次感更加鲜明。</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>类型#上衣*版型#宽松*版型#显瘦*风格#淑女*风格#简约*风格#知性*图案#条纹*图案#刺绣*衣样式#衬衫*衣长#中长款</td>\n",
              "      <td>衬衫采用中长款的宽松版型，散口的下摆能够巧妙地遮挡住腰腹部的赘肉，显瘦的同时更显飘逸。竖条纹的纹理设计更有现代简约风格，搭配上绣花图案的设计以后，还多了一份淑女优雅之感，彰显出女性婉约知性的气质魅力。</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "show_random_elements(dataset[\"train\"], num_examples=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "489ccc1b-3820-4f72-934d-3bb0875de954",
      "metadata": {
        "id": "489ccc1b-3820-4f72-934d-3bb0875de954"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "b875200f-7e13-46d8-954d-bfaa76829769",
      "metadata": {
        "id": "b875200f-7e13-46d8-954d-bfaa76829769"
      },
      "source": [
        "### 使用 ChatGLM3-6b Tokenizer 处理数据\n",
        "\n",
        "\n",
        "关于 `ignore_label_id` 的设置：\n",
        "\n",
        "在许多自然语言处理和机器学习框架中，`ignore_label_id` 被设置为 -100 是一种常见的约定。这个特殊的值用于标记在计算损失函数时应该被忽略的目标标签。让我们详细了解一下这个选择的原因：\n",
        "\n",
        "1. **损失函数忽略特定值**：训练语言模型时，损失函数（例如交叉熵损失）通常只计算对于模型预测重要或关键的标签的损失。在某些情况下，你可能不希望某些标签对损失计算产生影响。例如，在序列到序列的模型中，输入部分的标签通常被设置为一个忽略值，因为只有输出部分的标签对于训练是重要的。\n",
        "\n",
        "2. **为何选择-100**：这个具体的值是基于实现细节选择的。在 PyTorch 的交叉熵损失函数中，可以指定一个 `ignore_index` 参数。当损失函数看到这个索引值时，它就会忽略对应的输出标签。使用 -100 作为默认值是因为它是一个不太可能出现在标签中的数字（特别是在处理分类问题时，标签通常是从0开始的正整数）。\n",
        "\n",
        "3. **标准化和通用性**：由于这种做法在多个库和框架中被采纳，-100 作为忽略标签的默认值已经变得相对标准化，这有助于维护代码的通用性和可读性。\n",
        "\n",
        "总的来说，将 `ignore_label_id` 设置为 -100 是一种在计算损失时排除特定标签影响的便捷方式。这在处理特定类型的自然语言处理任务时非常有用，尤其是在涉及序列生成或修改的任务中。\n",
        "\n",
        "#### 关于 ChatGLM3 的填充处理说明\n",
        "\n",
        "- input_id（query）里的填充补全了输入长度，目的是不改变原始文本的含义。\n",
        "- label（answer）里的填充会用来跟模型基于 query 生成的结果计算 Loss，为了不影响损失值计算，也需要设置。咱们计算损失时，是针对 answer 部分的 Embedding Vector，因此 label 这样填充，前面的序列就自动忽略掉了，只比较生成内容的 loss。因此，需要将answer前面的部分做忽略填充。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "42e3c1f1-1e5b-4452-babe-234c8ae15bb5",
      "metadata": {
        "id": "42e3c1f1-1e5b-4452-babe-234c8ae15bb5",
        "outputId": "8448076e-0439-4035-bbf8-6c59ae2151d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# revision='b098244' 版本对应的 ChatGLM3-6B 设置 use_reentrant=False\n",
        "# 最新版本 use_reentrant 被设置为 True，会增加不必要的显存开销\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\n",
        "                                          trust_remote_code=True,\n",
        "                                          revision='b098244',\n",
        "                                          padding_side=\"left\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show transformers"
      ],
      "metadata": {
        "id": "9r7S97jzxML3",
        "outputId": "b5151a16-c460-44a3-d433-93075ec77fe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "9r7S97jzxML3",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: transformers\n",
            "Version: 4.37.2\n",
            "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
            "Home-page: https://github.com/huggingface/transformers\n",
            "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
            "Author-email: transformers@huggingface.co\n",
            "License: Apache 2.0 License\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
            "Required-by: peft, sentence-transformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2cc4a0fd-3239-4cb4-bf3f-8d8fcfff9af0",
      "metadata": {
        "id": "2cc4a0fd-3239-4cb4-bf3f-8d8fcfff9af0"
      },
      "outputs": [],
      "source": [
        "# tokenize_func 函数\n",
        "def tokenize_func(example, tokenizer, ignore_label_id=-100):\n",
        "    \"\"\"\n",
        "    对单个数据样本进行tokenize处理。\n",
        "\n",
        "    参数:\n",
        "    example (dict): 包含'content'和'summary'键的字典，代表训练数据的一个样本。\n",
        "    tokenizer (transformers.PreTrainedTokenizer): 用于tokenize文本的tokenizer。\n",
        "    ignore_label_id (int, optional): 在label中用于填充的忽略ID，默认为-100。\n",
        "\n",
        "    返回:\n",
        "    dict: 包含'tokenized_input_ids'和'labels'的字典，用于模型训练。\n",
        "    \"\"\"\n",
        "\n",
        "    # 构建问题文本\n",
        "    question = prompt_text + example['content']\n",
        "    if example.get('input', None) and example['input'].strip():\n",
        "        question += f'\\n{example[\"input\"]}'\n",
        "\n",
        "    # 构建答案文本\n",
        "    answer = example['summary']\n",
        "\n",
        "    # 对问题和答案文本进行tokenize处理\n",
        "    q_ids = tokenizer.encode(text=question, add_special_tokens=False)\n",
        "    a_ids = tokenizer.encode(text=answer, add_special_tokens=False)\n",
        "\n",
        "    # 如果tokenize后的长度超过最大长度限制，则进行截断\n",
        "    if len(q_ids) > max_input_length - 2:  # 保留空间给gmask和bos标记\n",
        "        q_ids = q_ids[:max_input_length - 2]\n",
        "    if len(a_ids) > max_output_length - 1:  # 保留空间给eos标记\n",
        "        a_ids = a_ids[:max_output_length - 1]\n",
        "\n",
        "    # 构建模型的输入格式\n",
        "    input_ids = tokenizer.build_inputs_with_special_tokens(q_ids, a_ids)\n",
        "    question_length = len(q_ids) + 2  # 加上gmask和bos标记\n",
        "\n",
        "    # 构建标签，对于问题部分的输入使用ignore_label_id进行填充\n",
        "    labels = [ignore_label_id] * question_length + input_ids[question_length:]\n",
        "\n",
        "    return {'input_ids': input_ids, 'labels': labels}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d576b3fa-156a-4486-a267-ad773172d90b",
      "metadata": {
        "id": "d576b3fa-156a-4486-a267-ad773172d90b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3a7cda56-a96a-4787-8db9-a712c82e2bb0",
      "metadata": {
        "id": "3a7cda56-a96a-4787-8db9-a712c82e2bb0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f78a19ed-6862-42fa-896d-2c297b94ce41",
      "metadata": {
        "id": "f78a19ed-6862-42fa-896d-2c297b94ce41",
        "outputId": "e7c1f3e6-3bfa-4325-c9bc-8dc24c6eb933",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396,
          "referenced_widgets": [
            "7eeaf0cd3b204e76a3a163bfb181e188",
            "db683038786c42c8a7117aefebbf2c87",
            "0b382d5a46a4471796da58c235347279",
            "5cb49b6a210f4eec9ac280c56ea343eb",
            "b39ea09d22134591bb1b3dda049e91b2",
            "1695d793e58d44f58f1c9d515c011c11",
            "9649cd3946ee46e9ab1b79044bd2c77e",
            "0d98ec8f91f5426987be9b6b7bc782eb",
            "558283c6016145dbb988e842a245a6b9",
            "c1eaab34350b4bdba79c4a0aaa781f43",
            "7156baeda54844f38b560d6f98e55cf9"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/114599 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7eeaf0cd3b204e76a3a163bfb181e188"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e4bf709f14eb>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcolumn_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m tokenized_dataset = dataset['train'].map(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mlambda\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenize_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mremove_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m         }\n\u001b[1;32m    559\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3071\u001b[0m                     \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"Map\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3072\u001b[0m                 ) as pbar:\n\u001b[0;32m-> 3073\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdataset_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3074\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m                             \u001b[0mshards_done\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3444\u001b[0m                     \u001b[0m_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3445\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshard_iterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3446\u001b[0;31m                         \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_function_on_filtered_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3447\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3448\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3336\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwith_rank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3337\u001b[0m                 \u001b[0madditional_args\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3338\u001b[0;31m             \u001b[0mprocessed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3339\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3340\u001b[0m                 processed_inputs = {\n",
            "\u001b[0;32m<ipython-input-11-e4bf709f14eb>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(example)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcolumn_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m tokenized_dataset = dataset['train'].map(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mlambda\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenize_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mremove_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-8d81aa7c20d5>\u001b[0m in \u001b[0;36mtokenize_func\u001b[0;34m(example, tokenizer, ignore_label_id)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# 对问题和答案文本进行tokenize处理\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mq_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0ma_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# 如果tokenize后的长度超过最大长度限制，则进行截断\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2572\u001b[0m                 method).\n\u001b[1;32m   2573\u001b[0m         \"\"\"\n\u001b[0;32m-> 2574\u001b[0;31m         encoded_inputs = self.encode_plus(\n\u001b[0m\u001b[1;32m   2575\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2576\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2980\u001b[0m         )\n\u001b[1;32m   2981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2982\u001b[0;31m         return self._encode_plus(\n\u001b[0m\u001b[1;32m   2983\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2984\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m             )\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mconvert_tokens_to_ids\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m             \u001b[0mids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_token_to_id_with_added_voc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_convert_token_to_id_with_added_voc\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_added_tokens_encoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_added_tokens_encoder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_token_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_convert_token_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm3-6b/b098244a71fbe69ce149682d9072a7629f7e908c/tokenization_chatglm.py\u001b[0m in \u001b[0;36m_convert_token_to_id\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_convert_token_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;34m\"\"\" Converts a token (str) in an id using the vocab. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_token_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_convert_id_to_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm3-6b/b098244a71fbe69ce149682d9072a7629f7e908c/tokenization_chatglm.py\u001b[0m in \u001b[0;36mconvert_token_to_id\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPieceToId\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert_id_to_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "column_names = dataset['train'].column_names\n",
        "tokenized_dataset = dataset['train'].map(\n",
        "    lambda example: tokenize_func(example, tokenizer),\n",
        "    batched=False,\n",
        "    remove_columns=column_names\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 异常：TypeError: ChatGLMTokenizer._pad() got an unexpected keyword argument 'padding_side'\n",
        "```bash\n",
        "TypeError                                 Traceback (most recent call last)\n",
        "<ipython-input-11-e4bf709f14eb> in <cell line: 2>()\n",
        "      1 column_names = dataset['train'].column_names\n",
        "----> 2 tokenized_dataset = dataset['train'].map(\n",
        "      3     lambda example: tokenize_func(example, tokenizer),\n",
        "      4     batched=False,\n",
        "      5     remove_columns=column_names\n",
        "\n",
        "10 frames\n",
        "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py in pad(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\n",
        "   3342         required_input = encoded_inputs[self.model_input_names[0]]\n",
        "   3343         if required_input and not isinstance(required_input[0], (list, tuple)):\n",
        "-> 3344             encoded_inputs = self._pad(\n",
        "   3345                 encoded_inputs,\n",
        "   3346                 max_length=max_length,\n",
        "\n",
        "TypeError: ChatGLMTokenizer._pad() got an unexpected keyword argument 'padding_side'\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "De2HJwbwydjp"
      },
      "id": "De2HJwbwydjp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fe0b50d-56cf-469f-9b35-0eb573cd1b04",
      "metadata": {
        "id": "6fe0b50d-56cf-469f-9b35-0eb573cd1b04"
      },
      "outputs": [],
      "source": [
        "show_random_elements(tokenized_dataset, num_examples=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18b28ee0-2207-41c8-a4cf-166d6c2b7a42",
      "metadata": {
        "id": "18b28ee0-2207-41c8-a4cf-166d6c2b7a42"
      },
      "source": [
        "### 数据集处理：shuffle & flatten\n",
        "\n",
        "洗牌(shuffle)会将数据集的索引列表打乱，以创建一个索引映射。\n",
        "\n",
        "然而，一旦您的数据集具有索引映射，速度可能会变慢10倍。这是因为需要额外的步骤来使用索引映射获取要读取的行索引，并且最重要的是，您不再连续地读取数据块。\n",
        "\n",
        "要恢复速度，需要再次使用 Dataset.flatten_indices()将整个数据集重新写入磁盘上，从而删除索引映射。\n",
        "\n",
        "ref: https://huggingface.co/docs/datasets/v2.15.0/en/package_reference/main_classes#datasets.Dataset.flatten_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75293898-f010-4cae-bf01-d58afc9dc214",
      "metadata": {
        "id": "75293898-f010-4cae-bf01-d58afc9dc214"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = tokenized_dataset.shuffle(seed=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99f17038-ec1a-4935-ae39-afca620c461d",
      "metadata": {
        "id": "99f17038-ec1a-4935-ae39-afca620c461d"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = tokenized_dataset.flatten_indices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5fc7921-95b1-45b5-a88e-2a80f9b25ceb",
      "metadata": {
        "id": "a5fc7921-95b1-45b5-a88e-2a80f9b25ceb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "281bac4d-4a57-4de3-991e-d62fbe15b6af",
      "metadata": {
        "id": "281bac4d-4a57-4de3-991e-d62fbe15b6af"
      },
      "source": [
        "### 定义 DataCollatorForChatGLM 类 批量处理数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "631a70f9-a75e-42d6-ad8b-5fd635db8d84",
      "metadata": {
        "id": "631a70f9-a75e-42d6-ad8b-5fd635db8d84"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "# DataCollatorForChatGLM 类\n",
        "class DataCollatorForChatGLM:\n",
        "    \"\"\"\n",
        "    用于处理批量数据的DataCollator，尤其是在使用 ChatGLM 模型时。\n",
        "\n",
        "    该类负责将多个数据样本（tokenized input）合并为一个批量，并在必要时进行填充(padding)。\n",
        "\n",
        "    属性:\n",
        "    pad_token_id (int): 用于填充(padding)的token ID。\n",
        "    max_length (int): 单个批量数据的最大长度限制。\n",
        "    ignore_label_id (int): 在标签中用于填充的ID。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pad_token_id: int, max_length: int = 2048, ignore_label_id: int = -100):\n",
        "        \"\"\"\n",
        "        初始化DataCollator。\n",
        "\n",
        "        参数:\n",
        "        pad_token_id (int): 用于填充(padding)的token ID。\n",
        "        max_length (int): 单个批量数据的最大长度限制。\n",
        "        ignore_label_id (int): 在标签中用于填充的ID，默认为-100。\n",
        "        \"\"\"\n",
        "        self.pad_token_id = pad_token_id\n",
        "        self.ignore_label_id = ignore_label_id\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __call__(self, batch_data: List[Dict[str, List]]) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        处理批量数据。\n",
        "\n",
        "        参数:\n",
        "        batch_data (List[Dict[str, List]]): 包含多个样本的字典列表。\n",
        "\n",
        "        返回:\n",
        "        Dict[str, torch.Tensor]: 包含处理后的批量数据的字典。\n",
        "        \"\"\"\n",
        "        # 计算批量中每个样本的长度\n",
        "        len_list = [len(d['input_ids']) for d in batch_data]\n",
        "        batch_max_len = max(len_list)  # 找到最长的样本长度\n",
        "\n",
        "        input_ids, labels = [], []\n",
        "        for len_of_d, d in sorted(zip(len_list, batch_data), key=lambda x: -x[0]):\n",
        "            pad_len = batch_max_len - len_of_d  # 计算需要填充的长度\n",
        "            # 添加填充，并确保数据长度不超过最大长度限制\n",
        "            ids = d['input_ids'] + [self.pad_token_id] * pad_len\n",
        "            label = d['labels'] + [self.ignore_label_id] * pad_len\n",
        "            if batch_max_len > self.max_length:\n",
        "                ids = ids[:self.max_length]\n",
        "                label = label[:self.max_length]\n",
        "            input_ids.append(torch.LongTensor(ids))\n",
        "            labels.append(torch.LongTensor(label))\n",
        "\n",
        "        # 将处理后的数据堆叠成一个tensor\n",
        "        input_ids = torch.stack(input_ids)\n",
        "        labels = torch.stack(labels)\n",
        "\n",
        "        return {'input_ids': input_ids, 'labels': labels}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "790fb86b-7662-4c49-8e3b-3db239c60838",
      "metadata": {
        "id": "790fb86b-7662-4c49-8e3b-3db239c60838"
      },
      "outputs": [],
      "source": [
        "# 准备数据整理器\n",
        "data_collator = DataCollatorForChatGLM(pad_token_id=tokenizer.pad_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff4124a3-e2a5-4e68-828b-3db3590d6f04",
      "metadata": {
        "id": "ff4124a3-e2a5-4e68-828b-3db3590d6f04"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f280438-3192-405d-a7bc-095831af197f",
      "metadata": {
        "id": "4f280438-3192-405d-a7bc-095831af197f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0a223fcf-630a-4978-ad96-bfe93c6581ea",
      "metadata": {
        "id": "0a223fcf-630a-4978-ad96-bfe93c6581ea"
      },
      "source": [
        "## 训练模型\n",
        "\n",
        "### 加载 ChatGLM3-6B 量化模型\n",
        "\n",
        "使用 `nf4` 量化数据类型加载模型，开启双量化配置，以`bf16`混合精度训练，预估显存占用接近4GB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1480790-6035-4212-bc64-4292f6a908ca",
      "metadata": {
        "id": "a1480790-6035-4212-bc64-4292f6a908ca"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, BitsAndBytesConfig\n",
        "\n",
        "_compute_dtype_map = {\n",
        "    'fp32': torch.float32,\n",
        "    'fp16': torch.float16,\n",
        "    'bf16': torch.bfloat16\n",
        "}\n",
        "\n",
        "# QLoRA 量化配置\n",
        "q_config = BitsAndBytesConfig(load_in_4bit=True,\n",
        "                              bnb_4bit_quant_type='nf4',\n",
        "                              bnb_4bit_use_double_quant=True,\n",
        "                              bnb_4bit_compute_dtype=_compute_dtype_map['bf16'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd8738b8-f510-4d9c-b789-d5afda67e39b",
      "metadata": {
        "id": "cd8738b8-f510-4d9c-b789-d5afda67e39b"
      },
      "source": [
        "### 加载模型\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6ab5f48-a7ce-492a-bb8b-4c0c3ff04bd0",
      "metadata": {
        "id": "c6ab5f48-a7ce-492a-bb8b-4c0c3ff04bd0"
      },
      "outputs": [],
      "source": [
        "# revision='b098244' 版本对应的 ChatGLM3-6B 设置 use_reentrant=False\n",
        "# 最新版本 use_reentrant 被设置为 True，会增加不必要的显存开销\n",
        "model = AutoModel.from_pretrained(model_name_or_path,\n",
        "                                  quantization_config=q_config,\n",
        "                                  device_map='auto',\n",
        "                                  trust_remote_code=True,\n",
        "                                  revision='b098244')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaabfe10-d762-498a-87f1-1506d2ea2fe9",
      "metadata": {
        "id": "aaabfe10-d762-498a-87f1-1506d2ea2fe9"
      },
      "outputs": [],
      "source": [
        "# 获取当前模型占用的 GPU显存（差值为预留给 PyTorch 的显存）\n",
        "memory_footprint_bytes = model.get_memory_footprint()\n",
        "memory_footprint_mib = memory_footprint_bytes / (1024 ** 2)  # 转换为 MiB\n",
        "\n",
        "print(f\"{memory_footprint_mib:.2f}MiB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b58f580-34c9-4f7f-b4cb-86548f25f2b1",
      "metadata": {
        "id": "9b58f580-34c9-4f7f-b4cb-86548f25f2b1"
      },
      "source": [
        "### 预处理量化模型\n",
        "\n",
        "预处理量化后的模型，使其可以支持低精度微调训练\n",
        "\n",
        "ref: https://huggingface.co/docs/peft/main/en/developer_guides/quantization#quantize-a-model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f59a712-4829-4929-b795-31ffdfab1761",
      "metadata": {
        "id": "5f59a712-4829-4929-b795-31ffdfab1761"
      },
      "outputs": [],
      "source": [
        "from peft import TaskType, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "kbit_model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21649f5e-3f31-44ff-ab0b-82ef1614491a",
      "metadata": {
        "id": "21649f5e-3f31-44ff-ab0b-82ef1614491a"
      },
      "source": [
        "### 自定义模型新增 Adapter\n",
        "\n",
        "当新的热门 transformer 网络架构（新模型）发布时，Huggingface 社区会尽力快速将它们添加到PEFT中。\n",
        "\n",
        "如果是 Hugging Face Transformers 库还未内置支持的模型，可以使用自定义模型的方式进行配置。\n",
        "\n",
        "具体来说，在初始化相应的微调配置类（例如`LoraConfig`）时，我们需要显式指定在哪些层新增适配器（Adapter），并将其设置正确。\n",
        "\n",
        "ref: https://huggingface.co/docs/peft/developer_guides/custom_models\n",
        "\n",
        "\n",
        "#### PEFT 适配模块设置\n",
        "\n",
        "\n",
        "在PEFT库的 [constants.py](https://github.com/huggingface/peft/blob/main/src/peft/utils/constants.py) 文件中定义了不同的 PEFT 方法，在各类大模型上的微调适配模块。\n",
        "\n",
        "通常，名称相同的模型架构也类似，应用微调方法时的适配器设置也几乎一致。\n",
        "\n",
        "例如，如果新模型架构是`mistral`模型的变体，并且您想应用 LoRA 微调。在 TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING中`mistral`包含[\"q_proj\", \"v_proj\"]。\n",
        "\n",
        "这表示说，对于`mistral`模型，LoRA 的 target_modules 通常是 [\"q_proj\", \"v_proj\"]。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4dd1cb6-c1b4-430b-b031-c842ddc683a6",
      "metadata": {
        "id": "b4dd1cb6-c1b4-430b-b031-c842ddc683a6"
      },
      "outputs": [],
      "source": [
        "from peft.utils import TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING\n",
        "\n",
        "target_modules = TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING['chatglm']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "517018b1-dff0-4caf-a1ff-21933f95e4b5",
      "metadata": {
        "id": "517018b1-dff0-4caf-a1ff-21933f95e4b5"
      },
      "outputs": [],
      "source": [
        "target_modules"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d8b7c08-931e-4764-9bfb-4230bc825561",
      "metadata": {
        "id": "7d8b7c08-931e-4764-9bfb-4230bc825561"
      },
      "source": [
        "### LoRA 适配器配置"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab44290b-7bf2-44f0-ac36-ef0d41527224",
      "metadata": {
        "id": "ab44290b-7bf2-44f0-ac36-ef0d41527224"
      },
      "outputs": [],
      "source": [
        "lora_config = LoraConfig(\n",
        "    target_modules=target_modules,\n",
        "    r=lora_rank,\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    bias='none',\n",
        "    inference_mode=False,\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bee2f80a-ccac-4c85-965e-ce5e5be528e8",
      "metadata": {
        "id": "bee2f80a-ccac-4c85-965e-ce5e5be528e8"
      },
      "outputs": [],
      "source": [
        "qlora_model = get_peft_model(kbit_model, lora_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cf0acae-9e20-4d30-9999-53c3079cc42c",
      "metadata": {
        "id": "9cf0acae-9e20-4d30-9999-53c3079cc42c"
      },
      "outputs": [],
      "source": [
        "qlora_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d84f15b-93e1-4d90-80b9-ead28cd98406",
      "metadata": {
        "id": "6d84f15b-93e1-4d90-80b9-ead28cd98406"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "d8043c51-fa42-4867-9382-103ad5b082f3",
      "metadata": {
        "id": "d8043c51-fa42-4867-9382-103ad5b082f3"
      },
      "source": [
        "### 训练超参数配置\n",
        "\n",
        "- 1个epoch表示对训练集的所有样本进行一次完整的训练。\n",
        "- `num_train_epochs` 表示要完整进行多少个 epochs 的训练。\n",
        "\n",
        "#### 关于使用 num_train_epochs 时，训练总步数 `steps` 的计算方法\n",
        "\n",
        "- 训练总步数： `total_steps = steps/epoch * num_train_epochs`\n",
        "- 每个epoch的训练步数：`steps/epoch = num_train_examples / (batch_size * gradient_accumulation_steps)`\n",
        "\n",
        "\n",
        "**以 `adgen` 数据集为例计算**\n",
        "\n",
        "```json\n",
        "DatasetDict({\n",
        "    train: Dataset({\n",
        "        features: ['content', 'summary'],\n",
        "        num_rows: 114599\n",
        "    })\n",
        "    validation: Dataset({\n",
        "        features: ['content', 'summary'],\n",
        "        num_rows: 1070\n",
        "    })\n",
        "})\n",
        "```\n",
        "\n",
        "代入超参数和配置进行计算：\n",
        "\n",
        "```python\n",
        "num_train_epochs = 1\n",
        "num_train_examples = 114599\n",
        "batch_size = 16\n",
        "gradient_accumulation_steps = 4\n",
        "\n",
        "\n",
        "steps = num_train_epochs * num_train_examples / (batch_size * gradient_accumulation_steps)\n",
        "      = 1 * 114599 / (16 * 4)\n",
        "      = 1790\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa527b7e-2dd6-46f0-ba7f-49f1430695a2",
      "metadata": {
        "id": "aa527b7e-2dd6-46f0-ba7f-49f1430695a2"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"models/{model_name_or_path}\",          # 输出目录\n",
        "    per_device_train_batch_size=16,                     # 每个设备的训练批量大小\n",
        "    gradient_accumulation_steps=4,                     # 梯度累积步数\n",
        "    # per_device_eval_batch_size=8,                      # 每个设备的评估批量大小\n",
        "    learning_rate=1e-3,                                # 学习率\n",
        "    num_train_epochs=1,                                # 训练轮数\n",
        "    lr_scheduler_type=\"linear\",                        # 学习率调度器类型\n",
        "    warmup_ratio=0.1,                                  # 预热比例\n",
        "    logging_steps=10,                                 # 日志记录步数\n",
        "    save_strategy=\"steps\",                             # 模型保存策略\n",
        "    save_steps=100,                                    # 模型保存步数\n",
        "    # evaluation_strategy=\"steps\",                       # 评估策略\n",
        "    # eval_steps=500,                                    # 评估步数\n",
        "    optim=\"adamw_torch\",                               # 优化器类型\n",
        "    fp16=True,                                        # 是否使用混合精度训练\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "563f6592-1dac-433e-8195-c24761350cd5",
      "metadata": {
        "id": "563f6592-1dac-433e-8195-c24761350cd5"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "        model=qlora_model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "        data_collator=data_collator\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1ba1de6-6e37-42a1-9ff0-4e926dee5a74",
      "metadata": {
        "id": "d1ba1de6-6e37-42a1-9ff0-4e926dee5a74"
      },
      "source": [
        "#### 训练参数（用于演示）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6164c2c8-47b4-4472-a192-952d8e425554",
      "metadata": {
        "id": "6164c2c8-47b4-4472-a192-952d8e425554"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_demo_args = TrainingArguments(\n",
        "    output_dir=f\"models/demo/{model_name_or_path}\",          # 输出目录\n",
        "    per_device_train_batch_size=16,                     # 每个设备的训练批量大小\n",
        "    gradient_accumulation_steps=4,                     # 梯度累积步数\n",
        "    learning_rate=1e-3,                                # 学习率\n",
        "    max_steps=100,                                     # 训练步数\n",
        "    lr_scheduler_type=\"linear\",                        # 学习率调度器类型\n",
        "    warmup_ratio=0.1,                                  # 预热比例\n",
        "    logging_steps=10,                                 # 日志记录步数\n",
        "    save_strategy=\"steps\",                             # 模型保存策略\n",
        "    save_steps=20,                                    # 模型保存步数\n",
        "    optim=\"adamw_torch\",                               # 优化器类型\n",
        "    fp16=True,                                        # 是否使用混合精度训练\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9655c7a7-75fd-4f0a-b190-e97a533c9bae",
      "metadata": {
        "id": "9655c7a7-75fd-4f0a-b190-e97a533c9bae"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "        model=qlora_model,\n",
        "        args=training_demo_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "        data_collator=data_collator\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7073aa2a-22f0-4ac1-ba93-5d8953ddb18f",
      "metadata": {
        "id": "7073aa2a-22f0-4ac1-ba93-5d8953ddb18f"
      },
      "source": [
        "### 开始训练\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07413af1-dff0-466a-adaa-00a80f327350",
      "metadata": {
        "id": "07413af1-dff0-466a-adaa-00a80f327350"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "320cd8a8-3690-4a2b-b725-f4c1316646ac",
      "metadata": {
        "id": "320cd8a8-3690-4a2b-b725-f4c1316646ac"
      },
      "outputs": [],
      "source": [
        "trainer.model.save_pretrained(f\"models/demo/{model_name_or_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9909f94a-b993-41b4-99f2-95a3b8b38c14",
      "metadata": {
        "id": "9909f94a-b993-41b4-99f2-95a3b8b38c14"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2111a492-d605-4b69-b98b-7db97e45f7f3",
      "metadata": {
        "id": "2111a492-d605-4b69-b98b-7db97e45f7f3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7eeaf0cd3b204e76a3a163bfb181e188": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db683038786c42c8a7117aefebbf2c87",
              "IPY_MODEL_0b382d5a46a4471796da58c235347279",
              "IPY_MODEL_5cb49b6a210f4eec9ac280c56ea343eb"
            ],
            "layout": "IPY_MODEL_b39ea09d22134591bb1b3dda049e91b2"
          }
        },
        "db683038786c42c8a7117aefebbf2c87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1695d793e58d44f58f1c9d515c011c11",
            "placeholder": "​",
            "style": "IPY_MODEL_9649cd3946ee46e9ab1b79044bd2c77e",
            "value": "Map:   3%"
          }
        },
        "0b382d5a46a4471796da58c235347279": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d98ec8f91f5426987be9b6b7bc782eb",
            "max": 114599,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_558283c6016145dbb988e842a245a6b9",
            "value": 3723
          }
        },
        "5cb49b6a210f4eec9ac280c56ea343eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1eaab34350b4bdba79c4a0aaa781f43",
            "placeholder": "​",
            "style": "IPY_MODEL_7156baeda54844f38b560d6f98e55cf9",
            "value": " 3723/114599 [00:07&lt;06:46, 272.49 examples/s]"
          }
        },
        "b39ea09d22134591bb1b3dda049e91b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1695d793e58d44f58f1c9d515c011c11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9649cd3946ee46e9ab1b79044bd2c77e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d98ec8f91f5426987be9b6b7bc782eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "558283c6016145dbb988e842a245a6b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1eaab34350b4bdba79c4a0aaa781f43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7156baeda54844f38b560d6f98e55cf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}